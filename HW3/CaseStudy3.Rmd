---
title: "Case Study 3"
author: 
- Justin Glommen
- Alex Hsieh
- Atharva Fulay
- Peter Yao

date: "2/23/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup
Setting up the dataset and the actual data realization information into accessible variables for analysis.

```{r cars}
# Assuming the data is from the same working directory as this file
gene <- read.table("hcmv.txt", header=TRUE)
data <- gene[,1]
# Actual data
N <- 229354     # Population size
n <- 296        # Sample number of palindromes
editGene <- gene
site.random <- editGene[["location"]]
# Display the data
site.random
```

# The Data
Here a strip plot is shown to visualize where palindromes are distributed, by laying out all possible palindrome location sites and displaying binary values representative of palindrome occurances.

```{r}
library(lattice)
stripplot(site.random, pch=16, cex=0.25, xlab="Palindrome Location (Actualized Data)")
```

# Testing

## Uniform Random Distribution by Simple Random Sampling
```{r}
# Pseudo data for simulation
set.seed(22217)     # Setting seed for the date at which this analysis was performed
gene <- seq(1, N)
# Produce simple random sample
site.random <- sample.int(N, size=n)
site.random
```

Plotting the uniform random distribution with a strip plot, we can compare it to the actualized data above.

```{r}
library(lattice)
stripplot(site.random, pch=16, cex=0.25, xlab="Palindrome Location (Simple Random Simulation)")
```

It's easy to notice that while although similar, the actualized data may appear to have a couple more dense clusters than what was generated via the simple sample. We will need to perform more testing to confirm whether these apparent clusters are statistically significant within the actualized data.

## Spacing
Here we create a graph to determine the usualness of spacing in the dataset.
```{r}
spacing <- {0}
for (i in 1:length(data)){
   spacing[i] <- data[i+1] - data[i];
}
spacing <- spacing[!is.na(spacing)]
plot(spacing, pch=20,col='red', main="Spacing of Palindrome locations", xlab = "Palindrome index", ylab = "Spacing in Base Pairs");
lines(spacing, y=NULL)
```

## Monte Carlo Uniform Simulation

### Generating the samples
Here we are generating one thousand instances of 296 simple randomly chosen uniformally distributed  vriables, in order to acquire distributions of desired parameters for testing.
```{r}
B <- 1000 # 1000 bootstrap uniform samples
monteCarloSampleDists <- matrix(data = NA, ncol = n, nrow = B)
monteCarloSampleDists.mean <- vector(mode="logical", length = B)
for (i in 1:B) {
  # Row is overall sample, column is data per sample
  monteCarloSampleDists[i,] <- sample.int(N,n)
  # Need to sort them in order to check consecutive palindromes
  monteCarloSampleDists[i,] <- monteCarloSampleDists[i,order(monteCarloSampleDists[i,])]
  monteCarloSampleDists.mean[i] <- mean(monteCarloSampleDists[i,])
}

#print(monteCarloSampleDists[1,])

hist(monteCarloSampleDists.mean)
```


### Expected Consecutive Palindrome Occurences
Now, we want to build some statistics surrounding the randomly distributed monte carlo simulation.
Here we will count the longest string of consecutive palindromes using the sorted data, and more.
```{r}
counts <- matrix(data = NA, nrow = B, ncol = n)
for (i in 1:B) {
  indexHighestCount <- 0
  tempFirstIndex <- 0
  # 0 indicates false, 1 indicates true
  booleanIsConsecutiveNow <- 0
  # Counting the amount of consecutive palindromes
  count <- 1           # Initialized to one since we're counting backwards down below in loop
  highestCount <- 1    # Used to track the highest count overall
  # Inefficient, but stable
  # Starts from 2 to compare to last element
  for (j in 2:n) {
    #monteCarloSampleDists[i,j]
    if (monteCarloSampleDists[ i, (j - 1) ] == ((monteCarloSampleDists[i,j]) - 1)) {
      if(booleanIsConsecutiveNow == 0) {
          tempFirstIndex <- (j - 1) # Index at first palindrome in at least 2 consecutive occurances
      }
      count <- (count + 1)
      booleanIsConsecutiveNow <- 1
    } 
    else {
      if (count > highestCount) {
        highestCount <- count
        indexHighestCount <- tempFirstIndex
      }
      count <- 1
      tempFirstIndex <- 0
      booleanIsConsecutiveNow <- 0
    }
    
  }
  # Store highest count into the counts array
  counts[i,1] <- highestCount
  # Store the index at which highestCount occured into the array also.
  counts[i,2] <- indexHighestCount
}

# Clearly, two consecutively is quite normal.
plot(counts[,1])
summary(counts[,1])
```

Now we have to run the same process on the actualized data, to determine if there's an unusually long string of palindromes to help us easily identify the replication site.
```{r}
indexHighestCount <- 0
tempFirstIndex <- 0
# 0 indicates false, 1 indicates true
booleanIsConsecutiveNow <- 0
# Counting the amount of consecutive palindromes
count <- 1           # Initialized to one since we're counting backwards down below in loop
highestCount <- 1    # Used to track the highest count overall
# Inefficient, but stable
# Starts from 2 to compare to last element
for (i in 2:n) {
  if (data[ (i - 1) ] == ((data[i]) - 1)) {
    if(booleanIsConsecutiveNow == 0) {
        tempFirstIndex <- (i - 1) # Index at first palindrome in at least 2 consecutive occurances
    }
    count <- (count + 1)
    booleanIsConsecutiveNow <- 1
  } 
  else {
    if (count > highestCount) {
      highestCount <- count
      indexHighestCount <- tempFirstIndex
    }
    count <- 1
    tempFirstIndex <- 0
    booleanIsConsecutiveNow <- 0
  }
    
}
# Print out the highest count, and its index.
highestCount
indexHighestCount
```

Unfortunately, our realized data matches the above statistics when it comes to consecutive occurrences of palindromes, since the max amount of consecutive occurrences matched that of the third quartile value above. This means there were no unusually long strings of consecutively occuring palindromes, therefore we'll have to resort to the poisson process in order to help us better determine unusual clusters for the replication site.

## Poisson Process

First, we'll generate a histogram forming the basis of the poisson distribution of clusters given fixed intervals of one hundred.
```{r}
k <- 100;         # The interval length for clusters of palindromes
n <- 296;
# Split the data into the clustered intervals
tab <- table(cut(data, breaks = seq(0, 230000, 
    length.out = k+1), include.lowest = TRUE));

counts <- as.vector(tab);

hist(counts, breaks=seq(0,14,by=1), col = rgb(1,0,0,0.5), 
     probability = TRUE,
     xlab = "number of palindromes inside an interval (~ 2300 bases)", 
     ylim = c(0,0.3), include.lowest = TRUE, right = FALSE);
lines(density(counts, adjust = 2), col = rgb(1,0,0,0.5))
Pois <- rpois(296, lambda = mean(counts))
hist(Pois, breaks=seq(0,13,by=1), col = rgb(0,0,1,0.5), probability = TRUE, add = TRUE,
     include.lowest = TRUE, right = FALSE);
lines(density(Pois, adjust = 2), col = rgb(0,0,1,0.5))
legend("topright", legend = c("data", "Poisson"), lty = c(1,1), col = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)));
```

### Finding the Cluster
Then, we'll calculate the highest likely amount of clusters within any one given interval, and compare it to the highest amount of palindromes we have in any one cluster in the actualized data.

```{r}
# Here, the 99.95th percentile is calculated to help determine upper outlier.
highestLikelyCluster <- qpois(.9995, mean(tab))   # Using sample statistic of lambda hat = x bar
print('The highest likely amount in a cluster is:')
highestLikelyCluster
print('and the highest amount of any cluster in our actualized data is:')
max(tab)
```

It's clear that with 99.95% probability, a maximum of ten palindromes will be in any one cluster. Due to our outlier of 13, and given it's the only above 10, it seems like a likely candidate for our replication site.


## Chi-Squared Test
First we perfom chi-squared test on the simulated data.
```{r}
regionsplit <- function(n.region, gene, site){
  count.int <- table(cut(site, breaks = seq(1, length(gene), length.out=n.region+1), include.lowest=TRUE))
  count.vector <- as.vector(count.int)
  count.tab <- table(count.vector)
  return (count.tab)
}

n.region <- 50
regionsplit(n.region, gene, site.random)

chisqtable <- function(n.region, site, N){
  n <- length(site)
  # lambda estimate
  lambda.est <- n/n.region
  # cut into n.region number of non-overlapping intervals
  count.int <- table(cut(site, breaks = seq(1, length(gene), length.out=n.region+1), include.lowest=TRUE))
  # get the count levels range
  count.vector <- as.vector(count.int)
  count.range <- max(count.vector) - min(count.vector) + 1
  
  # create contingency table
  table <- matrix(rep(NA, count.range*3), count.range, 3)
  for (i in 1:count.range){
    offset <- min(count.vector) - 1
    # first column = count level
    table[i, 1] <- i + offset
    # second column = observed count
    table[i, 2] <- sum(count.vector == i + offset)
    # third column = expected count
    if ((i + offset == min(count.vector)) && (min(count.vector) != 0))
      table[i, 3] <- ppois(i+offset, lambda.est)*n.region
    else if (i + offset == max(count.vector))
      table[i, 3] <- 1 - ppois(i + offset - 1, lambda.est)
    else
      table[i, 3] <- (ppois(i+offset, lambda.est) - ppois(i + offset - 1, lambda.est))*n.region
  }
  return (table)
}
site.random.tabtemp <- chisqtable(n.region, site.random, N)

site.random.tab <- matrix(rep(NA, 7*2), 7, 2)
site.random.tab[1,] <- colSums(site.random.tabtemp[1:2, 2:3])
site.random.tab[2:6,] <- site.random.tabtemp[3:7, 2:3]
site.random.tab[7,] <- colSums(site.random.tabtemp[7:9, 2:3])
site.random.stats <- sum((site.random.tab[,2] - site.random.tab[,1])^2/site.random.tab[,2])
pchisq(site.random.stats, 7 - 2, lower.tail=FALSE) #if lower.tail=TRUE then you're testing something else
```

Here we get a result of about .01.

We then again perform the chi-squared test on the real data.
```{r}
#actual data
N <- 229354
n <- 296
site.random <- editGene[["location"]]


library(lattice)
stripplot(site.random, pch=16, cex=0.25)

n.region <- 50

regionsplit <- function(n.region, gene, site){
  count.int <- table(cut(site, breaks = unique(seq(1, N, length.out=n.region+1)), include.lowest=TRUE))
  count.vector <- as.vector(count.int)
  count.tab <- table(count.vector)
  return (count.tab)
}

regionsplit(n.region, gene, site.random)

chisqtable <- function(n.region, site, N){
  n <- length(site)
  # lambda estimate
  lambda.est <- n/n.region
  # cut into n.region number of non-overlapping intervals
  count.int <- table(cut(site, breaks = unique(seq(1, N, length.out=n.region+1)), include.lowest=TRUE))
  # get the count levels range
  count.vector <- as.vector(count.int)
  count.range <- max(count.vector) - min(count.vector) + 1
  
  # create contingency table
  table <- matrix(rep(NA, count.range*3), count.range, 3)
  for (i in 1:count.range){
    offset <- min(count.vector) - 1
    # first column = count level
    table[i, 1] <- i + offset
    # second column = observed count
    table[i, 2] <- sum(count.vector == i + offset)
    # third column = expected count
    if ((i + offset == min(count.vector)) && (min(count.vector) != 0))
      table[i, 3] <- ppois(i+offset, lambda.est)*n.region
    else if (i + offset == max(count.vector))
      table[i, 3] <- 1 - ppois(i + offset - 1, lambda.est)
    else
      table[i, 3] <- (ppois(i+offset, lambda.est) - ppois(i + offset - 1, lambda.est))*n.region
  }
  return (table)
}
site.random.tabtemp <- chisqtable(n.region, site.random, N)

site.random.tab <- matrix(rep(NA, 7*2), 7, 2)
site.random.tab[1,] <- colSums(site.random.tabtemp[1:2, 2:3])
site.random.tab[2:6,] <- site.random.tabtemp[3:7, 2:3]
site.random.tab[7,] <- colSums(site.random.tabtemp[7:9, 2:3])
site.random.stats <- sum((site.random.tab[,2] - site.random.tab[,1])^2/site.random.tab[,2])
pchisq(site.random.stats, 7 - 2, lower.tail=FALSE) #if lower.tail=TRUE then you're testing something else
```

We here get a result of about .22.
